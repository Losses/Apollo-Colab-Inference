{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8NuwnC--VPO4"
      },
      "outputs": [],
      "source": [
        "#@markdown #Install\n",
        "%cd /content/\n",
        "!git clone https://github.com/JusperLee/Apollo.git && cd Apollo\n",
        "!mkdir /content/Apollo/model\n",
        "\n",
        "%cd /content/Apollo/model\n",
        "!wget https://huggingface.co/JusperLee/Apollo/resolve/main/pytorch_model.bin\n",
        "\n",
        "!pip install omegaconf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xFqIYcKxVXyd"
      },
      "outputs": [],
      "source": [
        "%cd /content/Apollo\n",
        "#@markdown #Inference\n",
        "input_file_path = '/content/input.wav' #@param {type:\"string\"}\n",
        "output_file_path = '/content/output.wav' #@param {type:\"string\"}\n",
        "chunk_size = 30 #@param {type:\"slider\", min:5, max:50, step:5}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import look2hear.models\n",
        "import soundfile as sf\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def load_audio(file_path):\n",
        "    audio, samplerate = librosa.load(file_path, mono=False, sr=44100)\n",
        "    print(f'INPUT audio.shape = {audio.shape} | samplerate = {samplerate}')\n",
        "    return torch.from_numpy(audio), samplerate\n",
        "\n",
        "def save_audio(file_path, audio, samplerate=44100):\n",
        "    sf.write(file_path, audio.T, samplerate, subtype=\"PCM_16\")\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    chunk = chunk.cuda()  # Ensure chunk is on the appropriate device\n",
        "    with torch.no_grad():\n",
        "        return model(chunk)\n",
        "\n",
        "def main(input_wav, output_wav):\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "\n",
        "    global model\n",
        "    model = look2hear.models.BaseModel.from_pretrain(\"/content/Apollo/model/pytorch_model.bin\", sr=44100, win=20, feature_dim=256, layer=6).cuda()\n",
        "\n",
        "    test_data, samplerate = load_audio(input_wav)\n",
        "\n",
        "    # Ensure the input data is always 2D\n",
        "    if test_data.ndim == 1:  # Mono\n",
        "        test_data = test_data.unsqueeze(0)  # Add a channel dimension\n",
        "\n",
        "    chunk_length = chunk_size * samplerate  # chunk_size seconds to samples\n",
        "    num_chunks = (test_data.shape[1] + chunk_length - 1) // chunk_length  # Calculate number of chunks\n",
        "\n",
        "    processed_chunks = []\n",
        "\n",
        "    for i in tqdm(range(num_chunks)):\n",
        "        start = i * chunk_length\n",
        "        end = min(start + chunk_length, test_data.shape[1])  # Handle last chunk\n",
        "        chunk = test_data[:, start:end]  # Get the current chunk\n",
        "\n",
        "        # Process the chunk\n",
        "        if chunk.shape[0] == 2:  # Stereo\n",
        "            left_channel = chunk[0].unsqueeze(0).unsqueeze(0).cuda()\n",
        "            right_channel = chunk[1].unsqueeze(0).unsqueeze(0).cuda()\n",
        "            out_left = process_chunk(left_channel)\n",
        "            out_right = process_chunk(right_channel)\n",
        "            out_stereo = torch.stack((out_left.squeeze(0).squeeze(0).cpu(), out_right.squeeze(0).squeeze(0).cpu()), dim=0)\n",
        "            processed_chunks.append(out_stereo)\n",
        "        else:  # Mono\n",
        "            out = process_chunk(chunk.unsqueeze(0).cuda())\n",
        "            processed_chunks.append(out.squeeze(0).squeeze(0).cpu())\n",
        "\n",
        "    # Concatenate all processed chunks\n",
        "    final_output = torch.cat(processed_chunks, dim=-1)\n",
        "    save_audio(output_wav, final_output, samplerate)\n",
        "    print(f'Success! Output file saved as {output_wav}')\n",
        "\n",
        "    # Memory clearing\n",
        "    model.cpu()\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "main(input_file_path, output_file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
